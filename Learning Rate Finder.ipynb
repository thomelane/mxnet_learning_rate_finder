{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "# Learning Rate Finder\n",
    "### _A method for choosing the Learning Rate for Stochastic Gradient Descent_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## What is the Learning Rate?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$\\theta_{j+1} = \\theta_{j} - \\alpha  J'(\\theta_{j})$$\n",
    "\n",
    "$$\\alpha = \\text{learning rate}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Conceptually, how should we set the Learning Rate?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<center>![png](https://www.jeremyjordan.me/content/images/2018/02/Screen-Shot-2018-02-24-at-11.47.09-AM.png)</center>\n",
    "<center>https://www.jeremyjordan.me/nn-learning-rate/</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Complex surface for neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "An example for VGG-56:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<center><img src=\"https://www.cs.umd.edu/~tomg/img/landscapes/noshort.png\" width=\"600px\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## What effects can we observe?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<center><img src=\"https://cdn-images-1.medium.com/max/1600/0*uIa_Dz3czXO5iWyI.\" width=\"600px\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 'Learning Rate Finder' Method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Cyclical Learning Rates for Training Neural Networks\" by Leslie Smith (2015)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Given an initialized network, a defined loss and a training dataset we take the following steps:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "1. train one batch at a time (a.k.a. an iteration)\n",
    "2. start with a very small learning rate (e.g. 0.000001) and slowly increase it every iteration\n",
    "3. record the training loss and continue until we see the training loss diverge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# 'Learning Rate Finder' Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<center><img src=\"https://raw.githubusercontent.com/dmlc/web-data/master/mxnet/doc/tutorials/lr_finder/finder_plot.png\" width=\"600px\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Structure training loop: from Epoch to Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import mxnet as mx\n",
    "\n",
    "# Set seed for reproducibility\n",
    "mx.random.seed(42),\n",
    "\n",
    "class Learner():\n",
    "    def __init__(self, net, data_loader, ctx):\n",
    "        \"\"\"\n",
    "        net: network (mx.gluon.Block)\n",
    "        data_loader: training data loader (mx.gluon.data.DataLoader)\n",
    "        ctx: context (mx.gpu or mx.cpu)\n",
    "        \"\"\"\n",
    "        self.net = net\n",
    "        self.data_loader = data_loader\n",
    "        self.ctx = ctx\n",
    "        # So we don't need to be in `for batch in data_loader` scope\n",
    "        # and can call for next batch in `iteration`\n",
    "        self.data_loader_iter = iter(self.data_loader)\n",
    "        self.net.initialize(mx.init.Xavier(), ctx=self.ctx)\n",
    "        self.loss_fn = mx.gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "        self.trainer = mx.gluon.Trainer(net.collect_params(), 'sgd', {'learning_rate': .001})\n",
    "        \n",
    "    def iteration(self, lr=None, take_step=True):\n",
    "        \"\"\"\n",
    "        lr: learning rate to use for iteration (float)\n",
    "        take_step: take trainer step to update weights (boolean)\n",
    "        \"\"\"\n",
    "        # Update learning rate if different this iteration\n",
    "        if lr and (lr != self.trainer.learning_rate):\n",
    "            self.trainer.set_learning_rate(lr)\n",
    "        # Get next batch, and move context (e.g. to GPU if set)\n",
    "        data, label = next(self.data_loader_iter)\n",
    "        data = data.as_in_context(self.ctx)\n",
    "        label = label.as_in_context(self.ctx)\n",
    "        # Standard forward and backward pass\n",
    "        with mx.autograd.record():\n",
    "            output = self.net(data)\n",
    "            loss = self.loss_fn(output, label)\n",
    "        loss.backward()     \n",
    "        # Update parameters\n",
    "        if take_step: self.trainer.step(data.shape[0])  \n",
    "        # Set and return loss.\n",
    "        # Although notice this is still an MXNet NDArray to avoid blocking\n",
    "        self.iteration_loss = mx.nd.mean(loss)\n",
    "        return self.iteration_loss\n",
    "\n",
    "    def close(self):\n",
    "        # Close open iterator and associated workers\n",
    "        self.data_loader_iter.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import mxnet as mx\n",
    "\n",
    "\n",
    "class Learner():\n",
    "    def __init__(self, net, data_loader, ctx):\n",
    "        \"\"\"\n",
    "        net: network (mx.gluon.Block)\n",
    "        data_loader: training data loader (mx.gluon.data.DataLoader)\n",
    "        ctx: context (mx.gpu or mx.cpu)\n",
    "        \"\"\"\n",
    "        self.net = net\n",
    "        self.data_loader = data_loader\n",
    "        self.ctx = ctx\n",
    "        # So we don't need to be in `for batch in data_loader` scope\n",
    "        # and can call for next batch in `iteration`\n",
    "        self.data_loader_iter = iter(self.data_loader)\n",
    "        self.net.initialize(mx.init.Xavier(), ctx=self.ctx)\n",
    "        self.loss_fn = mx.gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "        self.trainer = mx.gluon.Trainer(net.collect_params(), 'sgd', {'learning_rate': .001})\n",
    "        \n",
    "    ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "class Learner():\n",
    "    \n",
    "    ...\n",
    "    \n",
    "    def iteration(self, lr=None, take_step=True):\n",
    "        \"\"\"\n",
    "        lr: learning rate to use for iteration (float)\n",
    "        take_step: take trainer step to update weights (boolean)\n",
    "        \"\"\"\n",
    "        # Update learning rate if different this iteration\n",
    "        if lr and (lr != self.trainer.learning_rate):\n",
    "            self.trainer.set_learning_rate(lr)\n",
    "        # Get next batch, and move context (e.g. to GPU if set)\n",
    "        data, label = next(self.data_loader_iter)\n",
    "        data = data.as_in_context(self.ctx)\n",
    "        label = label.as_in_context(self.ctx)\n",
    "        # Standard forward and backward pass\n",
    "        with mx.autograd.record():\n",
    "            output = self.net(data)\n",
    "            loss = self.loss_fn(output, label)\n",
    "        loss.backward()     \n",
    "        # Update parameters\n",
    "        if take_step: self.trainer.step(data.shape[0])  \n",
    "        # Set and return loss.\n",
    "        # Although notice this is still an MXNet NDArray to avoid blocking\n",
    "        self.iteration_loss = mx.nd.mean(loss)\n",
    "        return self.iteration_loss\n",
    "    \n",
    "    ...  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "class Learner():\n",
    "    \n",
    "    ...\n",
    "    \n",
    "    def close(self):\n",
    "        # Close open iterator and associated workers\n",
    "        self.data_loader_iter.shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Continuous DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from multiprocessing import cpu_count\n",
    "from mxnet.gluon.data.vision import transforms\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    # Switches HWC to CHW, and converts to `float32`\n",
    "    transforms.ToTensor(),\n",
    "    # Channel-wise, using pre-computed means and stds \n",
    "    transforms.Normalize(mean=[0.4914, 0.4822, 0.4465],\n",
    "                         std=[0.2023, 0.1994, 0.2010])\n",
    "])\n",
    "\n",
    "dataset = mx.gluon.data.vision.datasets.CIFAR10(train=True).transform_first(transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "class ContinuousBatchSampler():\n",
    "    def __init__(self, sampler, batch_size):\n",
    "        self._sampler = sampler\n",
    "        self._batch_size = batch_size\n",
    "\n",
    "    def __iter__(self):\n",
    "        batch = []\n",
    "        while True:\n",
    "            for i in self._sampler:\n",
    "                batch.append(i)\n",
    "                if len(batch) == self._batch_size:\n",
    "                    yield batch\n",
    "                    batch = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "sampler = mx.gluon.data.RandomSampler(len(dataset))\n",
    "batch_sampler = ContinuousBatchSampler(sampler, batch_size=128)\n",
    "data_loader = mx.gluon.data.DataLoader(dataset, batch_sampler=batch_sampler, num_workers=cpu_count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 'Learning Rate Finder' Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/matplotlib/__init__.py:1067: UserWarning: Duplicate key in file \"/home/ubuntu/.config/matplotlib/matplotlibrc\", line #2\n",
      "  (fname, cnt))\n",
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/matplotlib/__init__.py:1067: UserWarning: Duplicate key in file \"/home/ubuntu/.config/matplotlib/matplotlibrc\", line #3\n",
      "  (fname, cnt))\n"
     ]
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "class LRFinder():\n",
    "    def __init__(self, learner):\n",
    "        \"\"\"\n",
    "        learner: able to take single iteration with given learning rate and return loss\n",
    "           and save and load parameters of the network (Learner)\n",
    "        \"\"\"\n",
    "        self.learner = learner\n",
    "        \n",
    "    def find(self, lr_start=1e-6, lr_multiplier=1.1, smoothing=0.3):\n",
    "        \"\"\"\n",
    "        lr_start: learning rate to start search (float)\n",
    "        lr_multiplier: factor the learning rate is multiplied by at each step of search (float)\n",
    "        smoothing: amount of smoothing applied to loss for stopping criteria (float)\n",
    "        \"\"\"\n",
    "        # Used to initialize weights; pass data, but don't take step.\n",
    "        # Would expect for new model with lazy weight initialization\n",
    "        self.learner.iteration(take_step=False)\n",
    "        # Used to initialize trainer (if no step has been taken)\n",
    "        if not self.learner.trainer._kv_initialized:\n",
    "            self.learner.trainer._init_kvstore()\n",
    "        # Store params and optimizer state for restore after lr_finder procedure\n",
    "        # Useful for applying the method partway through training, not just for initialization of lr.\n",
    "        self.learner.net.save_params(\"lr_finder.params\")\n",
    "        self.learner.trainer.save_states(\"lr_finder.state\")\n",
    "        lr = lr_start\n",
    "        self.results = [] # List of (lr, loss) tuples\n",
    "        stopping_criteria = LRFinderStoppingCriteria(smoothing)\n",
    "        while True:\n",
    "            # Run iteration, and block until loss is calculated.\n",
    "            loss = self.learner.iteration(lr).asscalar()\n",
    "            self.results.append((lr, loss))\n",
    "            if stopping_criteria(loss):\n",
    "                break\n",
    "            lr = lr * lr_multiplier\n",
    "        # Restore params (as finder changed them)\n",
    "        self.learner.net.load_params(\"lr_finder.params\", ctx=self.learner.ctx)\n",
    "        self.learner.trainer.load_states(\"lr_finder.state\")\n",
    "        self.plot()\n",
    "        \n",
    "    def plot(self):\n",
    "        lrs = [e[0] for e in self.results]\n",
    "        losses = [e[1] for e in self.results]\n",
    "        plt.figure(figsize=(6,8))\n",
    "        plt.scatter(lrs, losses)\n",
    "        plt.xlabel(\"Learning Rate\")\n",
    "        plt.ylabel(\"Loss\")\n",
    "        plt.xscale('log')\n",
    "        plt.yscale('log')\n",
    "        axes = plt.gca()\n",
    "        axes.set_xlim([lrs[0], lrs[-1]])\n",
    "        y_lower = min(losses) * 0.8\n",
    "        y_upper = losses[0] * 4\n",
    "        axes.set_ylim([y_lower, y_upper])\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "class LRFinder():\n",
    "    def __init__(self, learner):\n",
    "        \"\"\"\n",
    "        learner: able to take single iteration with given learning rate and return loss\n",
    "           and save and load parameters of the network (Learner)\n",
    "        \"\"\"\n",
    "        self.learner = learner\n",
    "        \n",
    "    ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "class LRFinder():\n",
    "    \n",
    "    ...\n",
    "    \n",
    "    def find(self, lr_start=1e-6, lr_multiplier=1.1, smoothing=0.3):\n",
    "        \"\"\"\n",
    "        lr_start: learning rate to start search (float)\n",
    "        lr_multiplier: factor the learning rate is multiplied by at each step of search (float)\n",
    "        smoothing: amount of smoothing applied to loss for stopping criteria (float)\n",
    "        \"\"\"\n",
    "        # Used to initialize weights; pass data, but don't take step.\n",
    "        # Would expect for new model with lazy weight initialization\n",
    "        self.learner.iteration(take_step=False)\n",
    "        # Used to initialize trainer (if no step has been taken)\n",
    "        if not self.learner.trainer._kv_initialized:\n",
    "            self.learner.trainer._init_kvstore()\n",
    "        # Store params and optimizer state for restore after lr_finder procedure\n",
    "        # Useful for applying the method partway through training, not just for initialization of lr.\n",
    "        self.learner.net.save_params(\"lr_finder.params\")\n",
    "        self.learner.trainer.save_states(\"lr_finder.state\")\n",
    "        lr = lr_start\n",
    "        self.results = [] # List of (lr, loss) tuples\n",
    "        stopping_criteria = LRFinderStoppingCriteria(smoothing)\n",
    "        while True:\n",
    "            # Run iteration, and block until loss is calculated.\n",
    "            loss = self.learner.iteration(lr).asscalar()\n",
    "            self.results.append((lr, loss))\n",
    "            if stopping_criteria(loss):\n",
    "                break\n",
    "            lr = lr * lr_multiplier\n",
    "        # Restore params (as finder changed them)\n",
    "        self.learner.net.load_params(\"lr_finder.params\", ctx=self.learner.ctx)\n",
    "        self.learner.trainer.load_states(\"lr_finder.state\")\n",
    "        self.plot()\n",
    "\n",
    "    ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "class LRFinder():\n",
    "    \n",
    "    ...\n",
    "       \n",
    "    def plot(self):\n",
    "        lrs = [e[0] for e in self.results]\n",
    "        losses = [e[1] for e in self.results]\n",
    "        plt.figure(figsize=(6,8))\n",
    "        plt.scatter(lrs, losses)\n",
    "        plt.xlabel(\"Learning Rate\")\n",
    "        plt.ylabel(\"Loss\")\n",
    "        plt.xscale('log')\n",
    "        plt.yscale('log')\n",
    "        axes = plt.gca()\n",
    "        axes.set_xlim([lrs[0], lrs[-1]])\n",
    "        y_lower = min(losses) * 0.8\n",
    "        y_upper = losses[0] * 4\n",
    "        axes.set_ylim([y_lower, y_upper])\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Stopping Criteria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "class LRFinderStoppingCriteria():\n",
    "    def __init__(self, smoothing=0.3, min_iter=20):\n",
    "        \"\"\"\n",
    "        smoothing: applied to running mean which is used for thresholding (float)\n",
    "        min_iter: minimum number of iterations before early stopping can occur (int)\n",
    "        \"\"\"\n",
    "        self.smoothing = smoothing\n",
    "        self.min_iter = min_iter\n",
    "        self.first_loss = None\n",
    "        self.running_mean = None\n",
    "        self.counter = 0\n",
    "        \n",
    "    def __call__(self, loss):\n",
    "        \"\"\"\n",
    "        loss: from single iteration (float)\n",
    "        \"\"\"\n",
    "        self.counter += 1\n",
    "        if self.first_loss is None:\n",
    "            self.first_loss = loss\n",
    "        if self.running_mean is None:\n",
    "            self.running_mean = loss\n",
    "        else:\n",
    "            self.running_mean = ((1 - self.smoothing) * loss) + (self.smoothing * self.running_mean)\n",
    "        return (self.running_mean > self.first_loss * 2) and (self.counter >= self.min_iter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 'Learning Rate Finder' Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x576 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ctx = mx.gpu() if mx.test_utils.list_gpus() else mx.cpu()\n",
    "net = mx.gluon.model_zoo.vision.resnet18_v2(classes=10)\n",
    "learner = Learner(net=net, data_loader=data_loader, ctx=ctx)\n",
    "lr_finder = LRFinder(learner)\n",
    "lr_finder.find(lr_start=1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0, Loss: 2.7886\n",
      "Iteration: 100, Loss: 1.6369\n",
      "Iteration: 200, Loss: 1.421\n",
      "Iteration: 300, Loss: 1.3718\n",
      "Iteration: 400, Loss: 1.2081\n",
      "Final Loss: 1.0667\n"
     ]
    }
   ],
   "source": [
    "learner.net.save_params(\"net.params\")\n",
    "lr = 0.05\n",
    "\n",
    "for iter_idx in range(500):\n",
    "    learner.iteration(lr=lr)\n",
    "    if ((iter_idx % 100) == 0):\n",
    "        print(\"Iteration: {}, Loss: {:.5g}\".format(iter_idx, learner.iteration_loss.asscalar()))\n",
    "print(\"Final Loss: {:.5g}\".format(learner.iteration_loss.asscalar()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0, Loss: 2.8218\n",
      "Iteration: 100, Loss: 1.8237\n",
      "Iteration: 200, Loss: 1.6513\n",
      "Iteration: 300, Loss: 1.3373\n",
      "Iteration: 400, Loss: 1.3635\n",
      "Final Loss: 1.1876\n"
     ]
    }
   ],
   "source": [
    "net = mx.gluon.model_zoo.vision.resnet18_v2(classes=10)\n",
    "learner = Learner(net=net, data_loader=data_loader, ctx=ctx)\n",
    "learner.net.load_params(\"net.params\", ctx=ctx)\n",
    "lr = 0.5\n",
    "\n",
    "for iter_idx in range(500):\n",
    "    learner.iteration(lr=lr)\n",
    "    if ((iter_idx % 100) == 0):\n",
    "        print(\"Iteration: {}, Loss: {:.5g}\".format(iter_idx, learner.iteration_loss.asscalar()))\n",
    "print(\"Final Loss: {:.5g}\".format(learner.iteration_loss.asscalar()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0, Loss: 2.7218\n",
      "Iteration: 100, Loss: 1.8424\n",
      "Iteration: 200, Loss: 1.7882\n",
      "Iteration: 300, Loss: 1.4884\n",
      "Iteration: 400, Loss: 1.2297\n",
      "Final Loss: 1.3336\n"
     ]
    }
   ],
   "source": [
    "net = mx.gluon.model_zoo.vision.resnet18_v2(classes=10)\n",
    "learner = Learner(net=net, data_loader=data_loader, ctx=ctx)\n",
    "learner.net.load_params(\"net.params\", ctx=ctx)\n",
    "lr = 0.005\n",
    "\n",
    "for iter_idx in range(500):\n",
    "    learner.iteration(lr=lr)\n",
    "    if ((iter_idx % 100) == 0):\n",
    "        print(\"Iteration: {}, Loss: {:.5g}\".format(iter_idx, learner.iteration_loss.asscalar()))\n",
    "print(\"Final Loss: {:.5g}\".format(learner.iteration_loss.asscalar()))"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Environment (conda_mxnet_p36)",
   "language": "python",
   "name": "conda_mxnet_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
